# -*- coding: utf-8 -*-
"""Problem_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x--wfkr1f4k4B72grY_TQNvfqUVCI3wU

**Mount Your Google Drive storage**

Enter the code using the link generating once you run the below cell
"""

# STEVEN CHU
# MICHAEL HAGGENMILLER

from google.colab import drive
drive.mount('/content/drive')

"""**Assuming that you have**

1.   Mounted your Google Drive storage
2.   Uploaded the data files on Google Drive
3.   Have the path to your uploaded folder

Say you have placed all your data in "ECE113" folder on Google Drive or which is the case, update the `enter_your_folder_name` in the path

Eg. 

```
%cd "drive/'My Drive'/ECE113" 
```
"""

# Commented out IPython magic to ensure Python compatibility.
# update this as per your folder name
# %cd "drive/My Drive/ECE 113 Project"

"""Make sure you are in the correct working directory"""

!pwd

"""Run the following code to verify whether you are in the correct directory on Google Colab"""

!ls

"""# Problem 1 
## Audio signal transformation pipeline for feature extraction
"""

# importing relevant libraries
from scipy.io import wavfile
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from glob import glob

# update this with the path of your training_data folder

PATH= './Problem_1_data/training_data_1'

audio_file_paths = [y for x in os.walk(PATH) for y in glob(os.path.join(x[0], '*.wav'))]

# since we have a total of 1994 chords i.e. ~200 files belonging to 10 classes
assert(len(audio_file_paths) == 1994)

len(audio_file_paths)

"""## Dataset Creation

**Processing each audio file from the 10 chords type placed in 'Problem_1_data/training_data_1' .**


---


Expect this code to take ~15 mins to run

---
"""

index = [i for i in range(len(audio_file_paths))]
columns = ['data', 'label']
df_train = pd.DataFrame(index=index, columns=columns)
for i, file_path in enumerate(audio_file_paths):
    fs, data = wavfile.read(file_path)
    # label assigned to each chord is the name of the folder it is placed inside
    label = os.path.dirname(file_path).split("/")[-1]
    df_train.loc[i] = [data, label]

y = df_train.iloc[:, 1].values
X = df_train.iloc[:, :-1].values
X = np.squeeze(X)
#X = np.stack(X, axis=0)

"""### Converting chord names into digits using LabelEncoder"""

from sklearn import preprocessing

labelencoder_y = preprocessing.LabelEncoder()
y = labelencoder_y.fit_transform(y)

"""You can verify the digits assigned to each chord type below"""

labelencoder_y.classes_
print(y)

"""## Tasks to perform

>  **(a)  Use the audio clip data to extract DFT features.  Plot the DFT for one chord from each chordtype (10 in total).  What do you observe?**
"""

######## PART A #########

indices = list(range(0,10))
tracking_indices = list()
y_indices = list()
# generate list of indices in the X data array to find one of each type of chord
for i in range(len(y)):
  if y[i] in indices:
    indices.remove(y[i])
    tracking_indices.append(y[i])
    y_indices.append(i)
chord_types = list()

# generate list of chord types corresponding to the order of the generated indices above
for i in range(len(tracking_indices)):
  chord_types.append(labelencoder_y.classes_[tracking_indices[i]])
  
# loop through the 10 sampled chords and plot the DFT spectra for each
for i in range(len(y_indices)):
  signal = X[i]
  N = len(signal)
  freq = np.linspace(0, 44100, num=N)
  DFT = np.fft.fft(signal)/N
  DFT = DFT[range(int(N/2))]
  DFT = np.abs(DFT)
  DFT = DFT/np.amax(DFT)
  values = np.arange(int(N/2))
  timePeriod = N/44100
  freq = values/timePeriod
  plt.plot(freq, DFT)
  plt.title("DFT of chord " + chord_types[i])
  plt.xlabel("Frequency")
  plt.show()

"""> **(b)  Create  a  function  to  convolve  any  unknown  chord  type  with existing  data  you  have  for  each chord to detect which chord it belongs to**"""

######## PART B #########

# compute the DFT of the crosscorrelation and find its *relative* energy
def crosscorr_energy(signal_1, signal_2):
  L = 0
  length1 = len(signal_1)
  length2 = len(signal_2)
  if(length1 > length2):
    L = length1
  else:
    L = length2
  f1 = np.fft.fft(signal_1, L)
  f2 = np.fft.fft(signal_2, L)
  crosscorr = np.multiply(f1, np.conj(f2))
  crosscorr = np.multiply(crosscorr, np.conj(crosscorr))
  energy = sum(crosscorr)
  return energy

# normalizing all the wavelets for fair comparison
X_norm = []
X_wavelets = []
for i in range(len(X)):
  norm_x = X[i]/np.linalg.norm(X[i])
  X_norm.append(norm_x)
for j in y_indices:
  X_wavelets.append(X_norm[j])

PATH= './Problem_1_data/test_data_1'
# max_length of audiofile

index = [i for i in range(10)]
columns = ['data']
df_test_1 = pd.DataFrame(index=index, columns=columns)
for i, file_path in enumerate(glob(os.path.join(PATH, '*.wav'))):
    fs, data = wavfile.read(file_path)
    df_test_1.loc[i] = [data]

X_test = df_test_1.iloc[:, :].values
X_test = np.squeeze(X_test)

"""Show the output of convolution between any two chord types from the training data"""

###### STILL PART B #######

# generate a confusion matrix
confusion_m = np.zeros(shape=(10,10))
# for each signal, carry out the cross correlation operation and comparison + update confusion matrix
for i in range(len(X_norm)):
  signal_1 = X_norm[i]
  actual_type = y[i]
  energies = []
  for j in range(len(X_wavelets)):
    signal_2 = X_wavelets[j]
    energy = crosscorr_energy(signal_1, signal_2)
    energies.append(energy)
  maxIndex = energies.index(np.max(energies))
  confusion_m[actual_type][tracking_indices[maxIndex]] 
  = confusion_m[actual_type][tracking_indices[maxIndex]] + 1

# plot the confusion matrix
plt.imshow(confusion_m)
plt.colorbar()
plt.title("Confusion Matrix of Training Data")
plt.xlabel("Predicted type")
plt.ylabel("Actual type")
plt.show()

"""> **(c) Compare the test audio files provided with the given 10 chords dataset to determine the chordtype  using  the  autocorrelation  operation.  Express the auto correlation of x[n] and h[n] as x[n]*h[-n] where * denotes convolution.   Create  a  matrix  in  which  each  column  and  each  row corresponds  to  a  chord  of  a  given  type.  For  each  test  case,  if  a  test  case  is  actually  of  chordtype  j  and  is  predicted  by  your  implementation  as  chord  type  i,  add  1  element  (i,j)  in  the matrix.  The more the confusion matrix looks like a diagonal matrix, the better the predictionalgorithm is.  This is called a confusion matrix.  Use `imshow(), imagesc()`, or another analogous command to plot the confusion matrix as an image.  Be sure to label the axes.**

*Run the auto correlation operation with existing audio files to find out the chord type of the 10 test examples given in `X_test`*
"""

###### PART C #######

# extracting the test files data
PATH= './Problem_1_data/test_data_1'

# max_length of audiofile
index = [i for i in range(10)]
columns = ['data']
df_test_1 = pd.DataFrame(index=index, columns=columns)
for i, file_path in enumerate(glob(os.path.join(PATH, '*.wav'))):
    fs, data = wavfile.read(file_path)
    df_test_1.loc[i] = [data]
# creating a list of the test signals
X_test = df_test_1.iloc[:, :].values
X_test = np.squeeze(X_test)


# normalize signals in X_test
x_test_norm = []
for i in range(len(X_test)):
  x_t_norm = X_test[i]/np.linalg.norm(X_test[i])
  x_test_norm.append(x_t_norm)

# run same cross correlation comparison with the test data signals using the same chosen wavelets
for i in range(len(x_test_norm)):
  signal_1 = x_test_norm[i]
  energies = []
  for j in range(len(X_wavelets)):
    signal_2 = X_wavelets[j]
    energy = crosscorr_energy(signal_1, signal_2)
    energies.append(energy)
  maxIndex = energies.index(np.max(energies))
  decision = "Test chord " + str(i) + " classified as: " + str(labelencoder_y.classes_[tracking_indices[maxIndex]])
  print(decision)

"""## From this section, you must turn in:


1.   All code
2.   A plot of the DFT of a single chord from each types and your observations for each part
3.   The results of your code given as a confusion matrix, along with some analysis of your results
4.   A suggestion for improving your accuracy

---
"""